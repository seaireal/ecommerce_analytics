---
title: "Understanding online shoppers' purchasing intention"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret) # subsampling
library(DMwR) # SMOTE sampling
library(ROSE) # synthetic data generation
library(glmnet) # logistic, ridge, and lasso
library(MASS) # lda, qda, stepaic
library(pROC) # ROC AUC plot
library(e1071) # svm
library(tree)
library(gbm) # boosting
library(randomForest)
library(nnet) # NN
```

# Data preparation
## Load data

```{r message=F}
df = read_csv("online_shoppers_intention.csv")
# unbalanced: less site visits converted to purchases
#table(df$Revenue)
```

## Formatting

```{r}
# convert categorical features to factor type, or SVM cross-validation can't work.

df = mutate(df, 
            Month = as.factor(Month),
            OperatingSystems = case_when(OperatingSystems > 3 ~ 0,
                                         TRUE ~ OperatingSystems) %>% as.factor(),
            Browser = case_when(Browser == 3 ~ 0,
                                Browser > 5 ~ 0,
                                TRUE ~ Browser) %>% as.factor(),
            Region = as.factor(Region),
            # combine levels with individual occurances less than 100
            TrafficType = case_when(TrafficType == 7 ~ 0,
                                    TrafficType == 9 ~ 0,
                                    TrafficType == 12 ~ 0,
                                    TrafficType == 14 ~ 0,
                                    TrafficType == 15 ~ 0,
                                    TrafficType == 16 ~ 0,
                                    TrafficType == 17 ~ 0,
                                    TrafficType == 18 ~ 0,
                                    TrafficType == 19 ~ 0,
                                    TRUE ~ TrafficType) %>% as.factor(),
            VisitorType = case_when(VisitorType != "Returning_Visitor" ~ "New_Visitor",
                                    TRUE ~ VisitorType) %>% as.factor(),
            Weekend = as.factor(Weekend),
            Revenue = case_when(Revenue == TRUE ~ 1,
                                Revenue == FALSE ~ 0) %>% as.factor())
```

## Training and test sets

```{r}
# 1:1 traning and test sets
set.seed(1)

split_size = as.integer(nrow(df)/2)
train_index = sample(1:nrow(df), split_size)
df_train = df[train_index, ]
df_test = df[-train_index, ]

df_train_original = df_train

table(df$Revenue)
sum(df$Revenue==1)/nrow(df)
table(df_train$Revenue)
sum(df_train$Revenue==1)/nrow(df_train)
table(df_test$Revenue)
sum(df_test$Revenue==1)/nrow(df_test)

#write_csv(df_train, "df_train.csv")
#write_csv(df_test, "df_test.csv")
```

## Subsampling on training set

Only run one of the following subsampling methods at a time before training models. Downsampling is accessed for all models. Random forest is selected to evaluate all subsampling methods.

### Downsampling

```{r}
# https://topepo.github.io/caret/subsampling-for-class-imbalances.html
set.seed(1)
df_train = downSample(x = df_train_original[, -ncol(df_train_original)], y = df_train_original$Revenue, yname = "Revenue")
table(df_train$Revenue) 
```

### Oversampling

```{r}
set.seed(1)
df_train = upSample(x = df_train_original[, -ncol(df_train_original)], y = df_train_original$Revenue, yname = "Revenue")
table(df_train$Revenue) 
```

### SMOTE

```{r}
set.seed(1)
df_train = SMOTE(Revenue ~ ., data  = as.data.frame(df_train_original))
table(df_train$Revenue)
```

### ROSE

```{r}
set.seed(1)
df_train <- ROSE(Revenue ~., data=df_train_original)$data
table(df_train$Revenue)
```

# Logistic regression
## Full model

```{r warning=F}
lgmod_full = glm(Revenue ~ ., df_train, family="binomial")
lgmod_full_pred <- predict(lgmod_full, df_test, type="response")
lgmod_full_revenue <- ifelse(lgmod_full_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_full_revenue, test_revenue)
print("test accuracy")
mean(lgmod_full_revenue == test_revenue)
print("sensitivity")
sum(lgmod_full_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_full_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_full_revenue, levels = c(0, 1), direction = "<") %>% auc()
```

## Stepwise selection

```{r warning=F}
lgmod_aic = stepAIC(lgmod_full, direction="backward", trace=FALSE)

colnames(df)
coef(lgmod_aic)
length(coef(lgmod_aic))

lgmod_aic_pred <- predict(lgmod_aic, df_test, type="response")
lgmod_aic_revenue <- ifelse(lgmod_aic_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue

print("confusion matrix")
table(lgmod_aic_revenue, test_revenue)
print("test accuracy")
mean(lgmod_aic_revenue == test_revenue)
print("sensitivity")
sum(lgmod_aic_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_aic_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_aic_pred, levels = c(0, 1), direction = "<") %>% auc()
```

## Lasso regularized

```{r}
# create model matrix
X_train_l = model.matrix(Revenue~., df_train)[ ,-1] # remove intercept
Y_train_l = df_train$Revenue
X_test_l = model.matrix(Revenue~., df_test)[ ,-1] # remove intercept
Y_test_l = df_test$Revenue
```

```{r}
set.seed(1)
lgmod_lasso_cv = cv.glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial")
lgmod_lasso = glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial", lambda = lgmod_lasso_cv$lambda.min)
#coef(lgmod_lasso)
lgmod_lasso_pred = predict(lgmod_lasso, s=lgmod_lasso_cv$lambda.min, newx=X_test_l, type="response")
lgmod_lasso_revenue <- ifelse(lgmod_lasso_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_lasso_revenue, test_revenue)
print("test accuracy")
mean(lgmod_lasso_revenue == test_revenue)
print("sensitivity")
sum(lgmod_lasso_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_lasso_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_lasso_pred[,1], levels = c(0, 1), direction = "<") %>% auc()
```

# SVM

## Linear kernel

Tuning returns the optimum cost as 1.

```{r}
# linear kernel SVM
set.seed(1)
#tune_svm1 = tune(svm, Revenue~., data = df_train, kernel = "linear", ranges = list(cost = c(0.1, 1, 10, 100), probability = TRUE))
#summary(tune_svm1)
svm1 = svm(Revenue~., data = df_train, kernel = "linear", cost=1, probability = TRUE)
           
predicted_svm1 = predict(svm1, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test accuracy")
mean(predicted_svm1 == test_revenue)

print("sensitivity")
sum(predicted_svm1==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm1==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm1)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

## Polynomial kernel

Tuning returns the optimum cost as 100.

```{r}
# polynomial kernel SVM
set.seed(1)
#tune_svm2 = tune(svm, Revenue~., data = df_train, kernel = "polynomial", ranges = list(cost = c(1, 10, 100, 1000), probability = TRUE))
#summary(tune_svm2)
svm2 = svm(Revenue~., data = df_train, kernel = "polynomial", degree = 3, cost=100, probability = TRUE)

predicted_svm2 = predict(svm2, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test accuracy")
mean(predicted_svm2 == test_revenue)

print("sensitivity")
sum(predicted_svm2==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm2==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm2)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

## Radial kernel

Tuning returns the optimum cost as 10 and gamma as 0.01.

```{r}
# radial kernel SVM
set.seed(1)
#tune_svm3 = tune(svm, Revenue~., data = df_train, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100),  gamma = c(0.001, 0.01, 0.1, 1), probability = TRUE))
#summary(tune_svm3)

svm3 = svm(Revenue~., data = df_train, kernel = "radial", gamma = 0.01, cost=10, probability = TRUE)

predicted_svm3 = predict(svm3, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test accuracy")
mean(predicted_svm3 == test_revenue)

print("sensitivity")
sum(predicted_svm3==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm3==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm3)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

Based on the above three SVM models, the radial kernel SVM provides the highest test accuracy of 90.02%

# Trees
## Decision Trees

```{r}
set.seed(1)
trmod = tree(Revenue~., df_train)
summary(trmod)
```

```{r}
plot(trmod)
text(trmod, pretty=0)
```

```{r}
set.seed(1)
trmod_cv = cv.tree(trmod)
trmod_cv
```

```{r}
set.seed(1)
trmod_prune = prune.tree(trmod, best = 5)
summary(trmod_prune)
```

```{r}
trmod_revenue = predict(trmod_prune, df_test, type="class")
print("confusion matrix")
table(trmod_revenue, test_revenue)
print("test accuracy")
mean(trmod_revenue==test_revenue)
print("sensitivity")
sum(trmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(trmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
trmod_pred = predict(trmod, df_test, type="vector")
roc(test_revenue, trmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()
```

## Gradient boosting
```{r}
df_train_b = mutate(df_train, Revenue=as.numeric(levels(Revenue))[Revenue])
df_test_b = mutate(df_test, Revenue=as.numeric(levels(Revenue))[Revenue])
set.seed(1)
bmod = gbm(Revenue~., df_train_b, distribution="bernoulli", n.trees=5000, interaction.depth=4)
```

```{r fig.height=8, fig.width=10}
bmod_imp = summary(bmod)
bmod_imp = bmod_imp %>% rename("predictor"="var", "rel_inf" = "rel.inf")
bmod_impgg = ggplot(bmod_imp, aes(reorder(predictor, rel_inf), rel_inf, fill=rel_inf)) + 
                              geom_col() + xlab(NULL) + ylab("relative influence") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
bmod_impgg
#ggsave("bmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
bmod_pred = predict(bmod, newdata=df_test_b, n.trees=5000, type="response")
bmod_revenue = ifelse(bmod_pred > 0.5, 1, 0)
print("confusion matrix")
table(bmod_revenue, test_revenue)
print("test accuracy")
mean(bmod_revenue==test_revenue)
print("sensitivity")
sum(bmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(bmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)
# auc
roc(test_revenue, bmod_pred, levels = c(0, 1), direction = "<") %>% auc()
```

## Random forest

```{r}
rfmod = randomForest(Revenue ~ ., data=df_train, importance=TRUE)
```

```{r}
test_revenue = df_test$Revenue
rfmod_revenue = predict(rfmod, newdata=df_test, type="response")
print("confusion matrix")
table(rfmod_revenue, test_revenue)
print("test accuracy")
mean(rfmod_revenue==test_revenue)
print("sensitivity")
sum(rfmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(rfmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
rfmod_pred=predict(rfmod, newdata=df_test, type="prob")
roc(test_revenue,rfmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()

```

```{r fig.height=8, fig.width=10}
rfmod_imp = importance(rfmod, type=1) %>% as.data.frame()
rfmod_imp = cbind(predictor = rownames(rfmod_imp), rfmod_imp)
rownames(rfmod_imp) = 1:nrow(rfmod_imp)
rfmod_imp = arrange(rfmod_imp, desc(MeanDecreaseAccuracy))
rfmod_impgg = ggplot(rfmod_imp, aes(reorder(predictor, MeanDecreaseAccuracy), MeanDecreaseAccuracy, fill=MeanDecreaseAccuracy)) + 
                              geom_col() + xlab(NULL) + ylab("mean decrease in accuracy") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
rfmod_impgg
#ggsave("rfmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
varImpPlot(rfmod)
```

# Neural network (single layer)
## Data preparation
```{r}
# convert categorical features to numeric type
df_train_c = mutate(df_train, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_train_c)
df_train_c_ohe <- as.data.frame(predict(dummies, newdata = df_train_c))
df_train_c <- cbind(df_train_c[,-c(which(colnames(df_train_c) %in% ohe_feats))],df_train_c_ohe)

#df_train_c[c(which(colnames(df_train_c) != "Revenue"))] = scale(df_train_c[c(which(colnames(df_train_c) != "Revenue"))])

df_test_c = mutate(df_test, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_test_c)
df_test_c_ohe <- as.data.frame(predict(dummies, newdata = df_test_c))
df_test_c <- cbind(df_test_c[,-c(which(colnames(df_test_c) %in% ohe_feats))],df_test_c_ohe)

#df_test_c[c(which(colnames(df_train_c) != "Revenue"))] = scale(df_test_c[c(which(colnames(df_train_c) != "Revenue"))])

#x_train = as.matrix(subset(df_train_c, select = -c(Revenue)))
#x_test = as.matrix(subset(df_test_c, select = -c(Revenue)))

#y_train = df_train_c$Revenue
#y_test = df_test_c$Revenue

#write_csv(df_train_c, "df_train_c.csv")
#write_csv(df_test_c, "df_test_c.csv")
```

## Parameter tuning

### Number of nodes

```{r}
set.seed(2)
node_num = c(5, 10, 15, 20, 25)
node_accuracy = array(0, dim=c(5, 10))
for(i in 1:5){
  for(j in 1:10){
    nn_node = nnet(Revenue ~ ., df_train_c, size = node_num[i], maxit = 100, trace=F)
    nn_node_pred = predict(nn_node, df_test_c, type="raw")[ ,1]
    nn_node_pred_label = ifelse(nn_node_pred > 0.5, 1, 0)
    node_accuracy[i, j] = mean(nn_node_pred_label == df_test_c$Revenue)
  }
}
```

```{r fig.height=5, fig.width=8}
#jpeg("node_opt.jpeg", res=300)
plot(as.factor(node_num) , node_accuracy, xlab="Number of Hidden Units", ylab="Test Accuracy")
#dev.off()
```

### Number of layers

Please refer to the Python code for this section.

### Weight decay

```{r}
set.seed(2)
lambda = c(0, 0.05, 0.1, 0.15, 0.2)
decay_accuracy = array(0,dim=c(5,10))
for (i in 1:5){
  for (j in 1:10){
    nn_decay = nnet(Revenue ~ ., df_train_c, size = 5, decay = lambda[i], maxit = 100, trace=F)
    nn_decay_pred = predict(nn_decay, df_test_c, type="raw")[ ,1]
    nn_decay_pred_label = ifelse(nn_decay_pred > 0.5, 1, 0)
    decay_accuracy[i, j] = mean(nn_decay_pred_label == df_test_c$Revenue)
  }
}
```

```{r fig.height=5, fig.width=8}

plot(as.factor(lambda) , decay_accuracy, xlab="Lambda", ylab="Test Accuracy")

```

## Model with optimized parameters

```{r}
set.seed(1)
nn_1_opt=nnet(Revenue ~ ., df_train_c, size = 5, decay = 0, maxit = 100, trace=F)
model.1.pred=predict(nn_1_opt, df_test_c, type="raw")[ ,1]
      
nn_1_opt.pred.label = ifelse(model.1.pred > 0.5, 1, 0)

print("test accuracy")
mean(nn_1_opt.pred.label == df_test_c$Revenue)
print("sensitivity")
sum(nn_1_opt.pred.label==1 & df_test_c$Revenue==1)/sum(df_test_c$Revenue==1)
print("specificity")
sum(nn_1_opt.pred.label==0 & df_test_c$Revenue==0)/sum(df_test_c$Revenue==0)

roc(df_test_c$Revenue, model.1.pred, levels = c(0, 1), direction = "<") %>% auc()
```