---
title: "Understanding online shoppers' purchasing intention"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret) # subsampling
library(DMwR) # SMOTE sampling
library(ROSE) # synthetic data generation
library(glmnet) # logistic, ridge, and lasso
library(MASS) # lda, qda, stepaic
library(pROC) # ROC AUC plot
library(e1071) # svm
library(tree)
library(gbm) # boosting
library(randomForest)
library(nnet) # NN
```

# Data preparation
## Load data

```{r message=F}
df = read_csv("online_shoppers_intention.csv")
# unbalanced: less site visits converted to purchases
#table(df$Revenue)
```

## Formatting

```{r}
# convert categorical features to factor type, or SVM cross-validation can't work.

df = mutate(df, 
            Month = as.factor(Month),
            OperatingSystems = as.factor(OperatingSystems),
            Browser = case_when(
              
            ),
            Region = as.factor(Region),
            # combine levels with individual occurances less than 100
            TrafficType = case_when(TrafficType == 7 ~ 0,
                                    TrafficType == 9 ~ 0,
                                    TrafficType == 12 ~ 0,
                                    TrafficType == 14 ~ 0,
                                    TrafficType == 15 ~ 0,
                                    TrafficType == 16 ~ 0,
                                    TrafficType == 17 ~ 0,
                                    TrafficType == 18 ~ 0,
                                    TrafficType == 19 ~ 0,
                                    TRUE ~ TrafficType) %>% as.factor(),
            VisitorType = as.factor(VisitorType),
            Weekend = as.factor(Weekend),
            Revenue = case_when(Revenue == TRUE ~ 1,
                                Revenue == FALSE ~ 0) %>% as.factor())
```

## Training and test sets

```{r}
# 1:1 traning and test sets
set.seed(1)

split_size = as.integer(nrow(df)/2)
train_index = sample(1:nrow(df), split_size)
df_train = df[train_index, ]
df_test = df[-train_index, ]

df_train_original = df_train

table(df$Revenue)
sum(df$Revenue==1)/nrow(df)
table(df_train$Revenue)
sum(df_train$Revenue==1)/nrow(df_train)
table(df_test$Revenue)
sum(df_test$Revenue==1)/nrow(df_test)

#write_csv(df_train, "df_train.csv")
#write_csv(df_test, "df_test.csv")
```

## Subsampling on training set
### Downsampling

```{r}
# https://topepo.github.io/caret/subsampling-for-class-imbalances.html
set.seed(1)
df_train = downSample(x = df_train_original[, -ncol(df_train_original)], y = df_train_original$Revenue, yname = "Revenue")
table(df_train$Revenue) 
```

### Oversampling

```{r}
set.seed(1)
df_train = upSample(x = df_train_original[, -ncol(df_train_original)], y = df_train_original$Revenue, yname = "Revenue")
table(df_train$Revenue) 
```

### SMOTE

```{r}
set.seed(1)
df_train = SMOTE(Revenue ~ ., data  = as.data.frame(df_train_original))
table(df_train$Revenue)
```

### ROSE

```{r}
set.seed(1)
df_train <- ROSE(Revenue ~., data=df_train_original)$data
table(df_train$Revenue)
```

# Logistic regression
## Full model

```{r warning=F}
lgmod_full = glm(Revenue ~ ., df_train, family="binomial")
lgmod_full_pred <- predict(lgmod_full, df_test, type="response")
lgmod_full_revenue <- ifelse(lgmod_full_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_full_revenue, test_revenue)
print("test error")
mean(lgmod_full_revenue != test_revenue)
print("sensitivity")
sum(lgmod_full_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_full_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_full_revenue, levels = c(0, 1), direction = "<") %>% auc()
```

## Stepwise selection

```{r warning=F}
lgmod_aic = stepAIC(lgmod_full, direction="backward", trace=FALSE)

colnames(df)
coef(lgmod_aic)
length(coef(lgmod_aic))

lgmod_aic_pred <- predict(lgmod_aic, df_test, type="response")
lgmod_aic_revenue <- ifelse(lgmod_aic_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue

print("confusion matrix")
table(lgmod_aic_revenue, test_revenue)
print("test error")
mean(lgmod_aic_revenue != test_revenue)
print("sensitivity")
sum(lgmod_aic_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_aic_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_aic_pred, levels = c(0, 1), direction = "<") %>% auc()
```

## Lasso regularized

```{r}
# create model matrix
X_train_l = model.matrix(Revenue~., df_train)[ ,-1] # remove intercept
Y_train_l = df_train$Revenue
X_test_l = model.matrix(Revenue~., df_test)[ ,-1] # remove intercept
Y_test_l = df_test$Revenue
```

```{r}
set.seed(1)
lgmod_lasso_cv = cv.glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial")
lgmod_lasso = glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial", lambda = lgmod_lasso_cv$lambda.min)
#coef(lgmod_lasso)
lgmod_lasso_pred = predict(lgmod_lasso, s=lgmod_lasso_cv$lambda.min, newx=X_test_l, type="response")
lgmod_lasso_revenue <- ifelse(lgmod_lasso_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_lasso_revenue, test_revenue)
print("test error")
mean(lgmod_lasso_revenue != test_revenue)
print("sensitivity")
sum(lgmod_lasso_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_lasso_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_lasso_pred[,1], levels = c(0, 1), direction = "<") %>% auc()
```

# SVM

```{r}
Accuracy = function(model){
  
predicted.train = predict(model, df_train)
predicted.test = predict(model, df_test)

training_accuracy = sum(predicted.train == df_train$Revenue)/length(df_train$Revenue)

test_accuracy = sum(predicted.test == df_test$Revenue)/length(df_test$Revenue)

both_accuracy = c(training_accuracy, test_accuracy)

return(both_accuracy)
}
```

## Linear kernel

```{r}
# linear kernel SVM
set.seed(3)
tune_svm1 = tune(svm, Revenue~., data = df_train, kernel = "linear", ranges = list(cost = c(0.1, 1, 10, 100), probability = TRUE))

summary(tune_svm1)

predicted_svm1 = predict(tune_svm1$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm1 != test_revenue)

print("sensitivity")
sum(predicted_svm1==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm1==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm1)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

## Polynomial kernel

```{r}
# polynomial kernel SVM
set.seed(3)
tune_svm2 = tune(svm, Revenue~., data = df_train, kernel = "polynomial", ranges = list(cost = c(1, 10, 100, 1000), probability = TRUE))

summary(tune_svm2)

predicted_svm2 = predict(tune_svm2$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm2 != test_revenue)

print("sensitivity")
sum(predicted_svm2==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm2==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm2)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

## Radial kernel

```{r}
# radial kernel SVM
set.seed(3)
tune_svm3 = tune(svm, Revenue~., data = df_train, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100),  gamma = c(0.001, 0.01, 0.1, 1), probability = TRUE))

summary(tune_svm3)

predicted_svm3 = predict(tune_svm3$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm3 != test_revenue)

print("sensitivity")
sum(predicted_svm3==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm3==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm3)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

Based on the above three SVM models, the radial kernel SVM provides the highest test accuracy of 90.02%

# Trees
## Decision Trees

```{r}
set.seed(1)
trmod = tree::tree(Revenue~., df_train)
summary(trmod)
```

```{r}
trmod_revenue = predict(trmod, df_test, type="class")
print("confusion matrix")
table(trmod_revenue, test_revenue)
print("test error rate")
mean(trmod_revenue!=test_revenue)
print("sensitivity")
sum(trmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(trmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
trmod_pred = predict(trmod, df_test, type="vector")
roc(test_revenue, trmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()

```

```{r}
plot(trmod)
text(trmod, pretty=0)
```

```{r}
set.seed(1)
trmod_cv = cv.tree(trmod)
trmod_cv
```

## Gradient boosting
```{r}
df_train_b = mutate(df_train, Revenue=as.numeric(levels(Revenue))[Revenue])
df_test_b = mutate(df_test, Revenue=as.numeric(levels(Revenue))[Revenue])
set.seed(1)
bmod = gbm(Revenue~., df_train_b, distribution="bernoulli", n.trees=5000, interaction.depth=4)
```

```{r fig.height=8, fig.width=10}
bmod_imp = summary(bmod)
bmod_imp = bmod_imp %>% rename("predictor"="var", "rel_inf" = "rel.inf")
bmod_impgg = ggplot(bmod_imp, aes(reorder(predictor, rel_inf), rel_inf, fill=rel_inf)) + 
                              geom_col() + xlab(NULL) + ylab("relative influence") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
bmod_impgg
#ggsave("bmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
bmod_pred = predict(bmod, newdata=df_test_b, n.trees=5000, type="response")
bmod_revenue = ifelse(bmod_pred > 0.5, 1, 0)
print("confusion matrix")
table(bmod_revenue, test_revenue)
print("test error rate")
mean(bmod_revenue!=test_revenue)
print("sensitivity")
sum(bmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(bmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)
# auc
roc(test_revenue, bmod_pred, levels = c(0, 1), direction = "<") %>% auc()
```

## Random forest

```{r}
rfmod = randomForest(Revenue ~ ., data=df_train, importance=TRUE)
```

```{r}
test_revenue = df_test$Revenue
rfmod_revenue = predict(rfmod, newdata=df_test, type="response")
print("confusion matrix")
table(rfmod_revenue, test_revenue)
print("test error rate")
mean(rfmod_revenue!=test_revenue)
print("sensitivity")
sum(rfmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(rfmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
rfmod_pred=predict(rfmod, newdata=df_test, type="prob")
roc(test_revenue,rfmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()

```

```{r fig.height=8, fig.width=10}
rfmod_imp = importance(rfmod, type=1) %>% as.data.frame()
rfmod_imp = cbind(predictor = rownames(rfmod_imp), rfmod_imp)
rownames(rfmod_imp) = 1:nrow(rfmod_imp)
rfmod_imp = arrange(rfmod_imp, desc(MeanDecreaseAccuracy))
rfmod_impgg = ggplot(rfmod_imp, aes(reorder(predictor, MeanDecreaseAccuracy), MeanDecreaseAccuracy, fill=MeanDecreaseAccuracy)) + 
                              geom_col() + xlab(NULL) + ylab("mean decrease in accuracy") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
rfmod_impgg
#ggsave("rfmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
varImpPlot(rfmod)
```

# Neural network (single layer)
## Data preparation
```{r}
# convert categorical features to numeric type
df_train_c = mutate(df_train, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_train_c)
df_train_c_ohe <- as.data.frame(predict(dummies, newdata = df_train_c))
df_train_c <- cbind(df_train_c[,-c(which(colnames(df_train_c) %in% ohe_feats))],df_train_c_ohe)

#df_train_c[c(which(colnames(df_train_c) != "Revenue"))] = scale(df_train_c[c(which(colnames(df_train_c) != "Revenue"))])

df_test_c = mutate(df_test, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_test_c)
df_test_c_ohe <- as.data.frame(predict(dummies, newdata = df_test_c))
df_test_c <- cbind(df_test_c[,-c(which(colnames(df_test_c) %in% ohe_feats))],df_test_c_ohe)

#df_test_c[c(which(colnames(df_train_c) != "Revenue"))] = scale(df_test_c[c(which(colnames(df_train_c) != "Revenue"))])

#x_train = as.matrix(subset(df_train_c, select = -c(Revenue)))
#x_test = as.matrix(subset(df_test_c, select = -c(Revenue)))

#y_train = df_train_c$Revenue
#y_test = df_test_c$Revenue

#write_csv(df_train_c, "df_train_c.csv")
#write_csv(df_test_c, "df_test_c.csv")
```

## parameter tuning

### Number of nodes

```{r}
set.seed(2)
node_num = c(5, 10, 15, 20, 25)
error_node = array(0, dim=c(5, 10))
for(i in 1:5){
  for(j in 1:10){
    nn_node = nnet(Revenue ~ ., df_train_c, size = node_num[i], maxit = 100, trace=F)
    nn_node_pred = predict(nn_node, df_test_c, type="raw")[ ,1]
    nn_node_pred_label = ifelse(nn_node_pred > 0.5, 1, 0)
    error_node[i, j] = mean(nn_node_pred_label != df_test_c$Revenue)
  }
}
```

```{r fig.height=5, fig.width=8}
#jpeg("node_opt.jpeg", res=300)
plot(as.factor(node_num) , error_node, xlab="Number of Hidden Units", ylab="Test Error")
#dev.off()
```

### Number of layers

Please refer to the Python code for this section.

### Weight decay

```{r}
set.seed(2)
lambda = c(0.01, 0.05, 0.1, 0.15, 0.2)
error_decay = array(0,dim=c(5,10))
for (i in 1:5){
  for (j in 1:10){
    nn_decay = nnet(Revenue ~ ., df_train_c, size = 5, decay = lambda[i], maxit = 100, trace=F)
    nn_decay_pred = predict(nn_decay, df_test_c, type="raw")[ ,1]
    nn_decay_pred_label = ifelse(nn_decay_pred > 0.5, 1, 0)
    error_decay[i, j] = mean(nn_decay_pred_label != df_test_c$Revenue)
  }
}
```

```{r fig.height=5, fig.width=8}

plot(as.factor(lambda) , error_decay, xlab="Lambda", ylab="Test Error")

```

## Model with optimized parameters

```{r}
set.seed(1)
model.1.nn=nnet(Revenue ~ ., df_train_c, size = 5, decay = 0.2, maxit = 100, trace=F)
model.1.pred=predict(model.1.nn, df_test_c, type="raw")[ ,1]
      
model.1.nn.pred.label = ifelse(model.1.pred > 0.5, 1, 0)

print("test error rate")
mean(model.1.nn.pred.label != df_test_c$Revenue)
print("sensitivity")
sum(model.1.nn.pred.label==1 & df_test_c$Revenue==1)/sum(df_test_c$Revenue==1)
print("specificity")
sum(model.1.nn.pred.label==0 & df_test_c$Revenue==0)/sum(df_test_c$Revenue==0)

roc(df_test_c$Revenue, model.1.pred, levels = c(0, 1), direction = "<") %>% auc()
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

