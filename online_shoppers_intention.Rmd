---
title: "Understanding online shoppers' purchasing intention"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet) # logistic, ridge, and lasso
library(MASS) # lda, qda, stepaic
library(pROC) # ROC AUC plot
library(e1071) # svm
library(DMwR) # sample unbalanced class
library(gbm) # boosting
library(tensorflow) #DNN
library(kerasR) # DNN
library(xgboost) # XGBoost
library(tree)
library(randomForest)
library(glasso)
library(caret) # misc
#library(igraph) # graph DAG
```

# Data preparation
## Load data

```{r}
df = read_csv("online_shoppers_intention.csv")
# unbalanced: less site visits converted to purchases
table(df$Revenue)
```
## Formatting

```{r}
# convert categorical features to factor type, or SVM cross-validation can't work.

df = mutate(df, 
            Month = as.factor(Month),
            OperatingSystems = as.factor(OperatingSystems),
            Browser = as.factor(Browser),
            Region = as.factor(Region),
            # combine levels with individual occurances less than 100
            TrafficType = case_when(TrafficType == 7 ~ 0,
                                    TrafficType == 9 ~ 0,
                                    TrafficType == 12 ~ 0,
                                    TrafficType == 14 ~ 0,
                                    TrafficType == 15 ~ 0,
                                    TrafficType == 16 ~ 0,
                                    TrafficType == 17 ~ 0,
                                    TrafficType == 18 ~ 0,
                                    TrafficType == 19 ~ 0,
                                    TRUE ~ TrafficType) %>% as.factor(),
            VisitorType = as.factor(VisitorType),
            Weekend = as.factor(Weekend),
            Revenue = case_when(Revenue == TRUE ~ 1,
                                Revenue == FALSE ~ 0) %>% as.factor())
```

## Under sampling

```{r}
# https://topepo.github.io/caret/subsampling-for-class-imbalances.html
set.seed(1)
df_0 = df[df$Revenue == 0, ]
df_1 = df[df$Revenue == 1, ]
df_0_sub = df_0[sample(1:nrow(df_0), nrow(df_1)), ]

split_size = as.integer(nrow(df_1)/2)
train_index = sample(1:nrow(df_1), split_size)

df_0_train = df_0_sub[train_index, ]
df_0_test = df_0_sub[-train_index, ]
df_1_train = df_1[train_index, ]
df_1_test = df_1[-train_index, ]

df_train = rbind(df_0_train, df_1_train)
df_test = rbind(df_0_test, df_1_test)

table(df_train$Revenue)
table(df_test$Revenue)
```

## Regular sampling

```{r}
set.seed(1)
df_0 = df[df$Revenue == 0, ]
df_1 = df[df$Revenue == 1, ]


split_size = as.integer(nrow(df_1)/5)
t_index = sample(1:nrow(df_1), split_size)
df_1_train = df_1[-t_index, ]
df_1_test = df_1[t_index, ]

split_size = as.integer(nrow(df_0)/5)
f_index = sample(1:nrow(df_0), split_size)
df_0_train = df_0[-f_index, ]
df_0_test = df_0[f_index, ]

df_train = rbind(df_0_train, df_1_train)
df_test = rbind(df_0_test, df_1_test)

table(df_train$Revenue)
table(df_test$Revenue)
```

## SMOTE: synthetic data generation
```{r}
balanced.data <- SMOTE(Revenue ~., df, perc.over = 8000,perc.under=100)

as.data.frame(table(balanced.data$Revenue))
```

# Logistic regression
## Full model

```{r warning=F}
lgmod_full = glm(Revenue ~ ., df_train, family="binomial")
lgmod_full_pred <- predict(lgmod_full, df_test, type="response")
lgmod_full_revenue <- ifelse(lgmod_full_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_full_revenue, test_revenue)
print("test error")
mean(lgmod_full_revenue != test_revenue)
print("sensitivity")
sum(lgmod_full_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_full_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_full_revenue, levels = c(0, 1), direction = "<") %>% auc()
```

## Stepwise selection

```{r warning=F}
lgmod_aic = stepAIC(lgmod_full, direction="backward", trace=FALSE)

colnames(df)
coef(lgmod_aic)
length(coef(lgmod_aic))

lgmod_aic_pred <- predict(lgmod_aic, df_test, type="response")
lgmod_aic_revenue <- ifelse(lgmod_aic_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue

print("confusion matrix")
table(lgmod_aic_revenue, test_revenue)
print("test error")
mean(lgmod_aic_revenue != test_revenue)
print("sensitivity")
sum(lgmod_aic_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_aic_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_aic_pred, levels = c(0, 1), direction = "<") %>% auc()
```

## Lasso regularized

```{r}
# create model matrix
X_train_l = model.matrix(Revenue~., df_train)[ ,-1] # remove intercept
Y_train_l = df_train$Revenue
X_test_l = model.matrix(Revenue~., df_test)[ ,-1] # remove intercept
Y_test_l = df_test$Revenue
```

```{r}
set.seed(1)
lgmod_lasso_cv = cv.glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial")
lgmod_lasso = glmnet(X_train_l, Y_train_l, alpha = 1, family = "binomial", lambda = lgmod_lasso_cv$lambda.min)
#coef(lgmod_lasso)
lgmod_lasso_pred = predict(lgmod_lasso, s=lgmod_lasso_cv$lambda.min, newx=X_test_l, type="response")
lgmod_lasso_revenue <- ifelse(lgmod_lasso_pred > 0.5, 1, 0)
test_revenue = df_test$Revenue
print("confusion matrix")
table(lgmod_lasso_revenue, test_revenue)
print("test error")
mean(lgmod_lasso_revenue != test_revenue)
print("sensitivity")
sum(lgmod_lasso_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(lgmod_lasso_revenue==0 & test_revenue==0)/sum(test_revenue==0)

#auc
roc(test_revenue, lgmod_lasso_pred[,1], levels = c(0, 1), direction = "<") %>% auc()
```

# SVM
```{r}
Accuracy = function(model){
  
predicted.train = predict(model, df_train)
predicted.test = predict(model, df_test)

training_accuracy = sum(predicted.train == df_train$Revenue)/length(df_train$Revenue)

test_accuracy = sum(predicted.test == df_test$Revenue)/length(df_test$Revenue)

both_accuracy = c(training_accuracy, test_accuracy)

return(both_accuracy)
}
```

```{r}
# linear kernel SVM
set.seed(3)
tune_svm1 = tune(svm, Revenue~., data = df_train, kernel = "linear", ranges = list(cost = c(0.1, 1, 10, 100), probability = TRUE))

summary(tune_svm1)

predicted_svm1 = predict(tune_svm1$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm1 != test_revenue)

print("sensitivity")
sum(predicted_svm1==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm1==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm1)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```


```{r}
# polynomial kernel SVM
set.seed(3)
tune_svm2 = tune(svm, Revenue~., data = df_train, kernel = "polynomial", ranges = list(cost = c(1, 10, 100, 1000), probability = TRUE))

summary(tune_svm2)

predicted_svm2 = predict(tune_svm2$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm2 != test_revenue)

print("sensitivity")
sum(predicted_svm2==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm2==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm2)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

```{r}
# radial kernel SVM
set.seed(3)
tune_svm3 = tune(svm, Revenue~., data = df_train, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100),  gamma = c(0.001, 0.01, 0.1, 1), probability = TRUE))

summary(tune_svm3)

predicted_svm3 = predict(tune_svm3$best.model, df_test, probability = TRUE)
test_revenue = df_test$Revenue

print("test error")
mean(predicted_svm3 != test_revenue)

print("sensitivity")
sum(predicted_svm3==1 & test_revenue==1)/sum(test_revenue==1)

print("specificity")
sum(predicted_svm3==0 & test_revenue==0)/sum(test_revenue==0)

roc(test_revenue, attributes(predicted_svm3)$probabilities[,2], levels = c(0, 1), direction = "<") %>% auc()
```

```{r}
# try a different sampling method
# linear kernel SVM
set.seed(3)
tune_svm1 = tune(svm, Revenue~., data = df_train, kernel = "linear", ranges = list(cost = c(0.1, 1, 10, 100)))

Accuracy(tune_svm1$best.model)

# polynomial kernel SVM
tune_svm2 = tune(svm, Revenue~., data = df_train, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))

Accuracy(tune_svm2$best.model)

# radial kernel SVM
tune_svm3 = tune(svm, Revenue~., data = df_train, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 10, 100),  gamma = c(0.1, 0.5, 1, 2,3,4,5)))

Accuracy(tune_svm3$best.model)
```

Based on the above three SVM models, the radial kernel SVM provides the highest test accuracy of 90.02%

# Graphical lasso: feature selection

```{r}
S=cov(df[, !sapply(df, is.factor)]) # for columns not character (-Month,-VisitorType)
dim(S)

fit1=glasso(S,rho=50)
Theta=fit1$wi
colnames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
rownames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
Adj=Theta!=0
Adj=Adj*1
diag(Adj)=0
plot(graph_from_adjacency_matrix(Adj, mode = c("undirected")))

fit2=glasso(S,rho=20)
Theta=fit2$wi
colnames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
rownames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
Adj=Theta!=0
Adj=Adj*1
diag(Adj)=0
plot(graph_from_adjacency_matrix(Adj, mode = c("undirected")))

fit3=glasso(S,rho=5)
Theta=fit3$wi
colnames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
rownames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
Adj=Theta!=0
Adj=Adj*1
diag(Adj)=0
plot(graph_from_adjacency_matrix(Adj, mode = c("undirected")))

fit4=glasso(S,rho=1)
Theta=fit4$wi
colnames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
rownames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
Adj=Theta!=0
Adj=Adj*1
diag(Adj)=0
plot(graph_from_adjacency_matrix(Adj, mode = c("undirected")))

fit5=glasso(S,rho=100)
Theta=fit5$wi
colnames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
rownames(Theta)=c("Administrative","Administrative_Duration","Informational","Informational_Duration","ProductRelated","ProductRelated_Duration","BounceRates","ExitRates","PageValues","SpecialDay")
Adj=Theta!=0
Adj=Adj*1
diag(Adj)=0
plot(graph_from_adjacency_matrix(Adj, mode = c("undirected")))
```

# Trees
## Decision Trees

```{r}
set.seed(1)
trmod = tree::tree(Revenue~., df_train)
summary(trmod)
```

```{r}
trmod_revenue = predict(trmod, df_test, type="class")
print("confusion matrix")
table(trmod_revenue, test_revenue)
print("test error rate")
mean(trmod_revenue!=test_revenue)
print("sensitivity")
sum(trmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(trmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
trmod_pred = predict(trmod, df_test, type="vector")
roc(test_revenue, trmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()

```

```{r}
plot(trmod)
text(trmod, pretty=0)
```

```{r}
set.seed(1)
trmod_cv = cv.tree(trmod)
trmod_cv
```

## Random forest

```{r}
rfmod = randomForest(Revenue ~ ., data=df_train, importance=TRUE)
```

```{r}
rfmod_revenue = predict(rfmod, newdata=df_test, type="response")
print("confusion matrix")
table(rfmod_revenue, test_revenue)
print("test error rate")
mean(rfmod_revenue!=test_revenue)
print("sensitivity")
sum(rfmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(rfmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)

# auc
rfmod_pred=predict(rfmod, newdata=df_test, type="prob")
roc(test_revenue,rfmod_pred[,2], levels = c(0, 1), direction = "<") %>% auc()

```

```{r fig.height=8, fig.width=10}
rfmod_imp = importance(rfmod, type=1) %>% as.data.frame()
rfmod_imp = cbind(predictor = rownames(rfmod_imp), rfmod_imp)
rownames(rfmod_imp) = 1:nrow(rfmod_imp)
rfmod_imp = arrange(rfmod_imp, desc(MeanDecreaseAccuracy))
rfmod_impgg = ggplot(rfmod_imp, aes(reorder(predictor, MeanDecreaseAccuracy), MeanDecreaseAccuracy, fill=MeanDecreaseAccuracy)) + 
                              geom_col() + xlab(NULL) + ylab("mean decrease in accuracy") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
rfmod_impgg
#ggsave("rfmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
varImpPlot(rfmod)
```

# Boosting

```{r}
df_train_b = mutate(df_train, Revenue=as.numeric(levels(Revenue))[Revenue])
df_test_b = mutate(df_test, Revenue=as.numeric(levels(Revenue))[Revenue])
set.seed(1)
bmod = gbm(Revenue~., df_train_b, distribution="bernoulli", n.trees=5000, interaction.depth=4)
```


```{r fig.height=8, fig.width=10}
bmod_imp = summary(bmod)
bmod_imp = bmod_imp %>% rename("predictor"="var", "rel_inf" = "rel.inf")
bmod_impgg = ggplot(bmod_imp, aes(reorder(predictor, rel_inf), rel_inf, fill=rel_inf)) + 
                              geom_col() + xlab(NULL) + ylab("relative influence") + coord_flip() +
                              scale_fill_continuous(name=NULL) + theme_classic() +
                              theme(text=element_text(size=18), legend.position=c(0.9, 0.2))
bmod_impgg
#ggsave("bmod_impgg.jpeg", width=10, height=8, units="in", dpi=300)
```

```{r}
bmod_pred = predict(bmod, newdata=df_test_b, n.trees=5000, type="response")
bmod_revenue = ifelse(bmod_pred > 0.5, 1, 0)
print("confusion matrix")
table(bmod_revenue, test_revenue)
print("test error rate")
mean(bmod_revenue!=test_revenue)
print("sensitivity")
sum(bmod_revenue==1 & test_revenue==1)/sum(test_revenue==1)
print("specificity")
sum(bmod_revenue==0 & test_revenue==0)/sum(test_revenue==0)
# auc
roc(test_revenue, bmod_pred, levels = c(0, 1), direction = "<") %>% auc()
```
# XGboost
```{r}
# convert features to numeric type, or XGBoost can't work.

df_train_c = mutate(df_train, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_train_c)
df_train_c_ohe <- as.data.frame(predict(dummies, newdata = df_train_c))
df_train_c <- cbind(df_train_c[,-c(which(colnames(df_train_c) %in% ohe_feats))],df_train_c_ohe)


df_test_c = mutate(df_test, 
            OperatingSystems = as.numeric(OperatingSystems),
            Browser = as.numeric(Browser),
            Region = as.numeric(Region),
            TrafficType = as.numeric(TrafficType),
            Weekend = as.numeric(Weekend),
            Revenue=as.numeric(levels(Revenue))[Revenue])
ohe_feats = c('Month', 'VisitorType')
dummies <- dummyVars(~ Month +  VisitorType, data = df_test_c)
df_test_c_ohe <- as.data.frame(predict(dummies, newdata = df_test_c))
df_test_c <- cbind(df_test_c[,-c(which(colnames(df_test_c) %in% ohe_feats))],df_test_c_ohe)
#lapply(df_test_c,class)

set.seed(1)
bst <- xgboost(data = data.matrix(df_train_c[ ,-16]), label = df_train_c$Revenue, max.depth = 15, eta = 1, nthread = 10, nrounds = 25, objective = "binary:logistic", verbose = 2)
pred <- predict(bst, data.matrix(df_test_c[ ,-16]))
err <- mean(as.numeric(pred > 0.5) != df_test_c$Revenue)
print(paste("test-error=", err))
```

```{r}
up_train <- upSample(x = imbal_train[, -ncol(imbal_train)],
                     y = imbal_train$Class)                         
table(up_train$Class) 
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```